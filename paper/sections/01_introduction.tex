% -----------------------------------------------------------------------------
\section{Introduction}
\label{sec:introduction}
% -----------------------------------------------------------------------------
Large language models (LLMs) routinely rely on Fourier projections to globally mix context.  Yet the Discrete Fourier Transform (DFT) is but a single point in a three-parameter family of \emph{Linear Canonical Transforms} (LCTs).  We argue that allowing a network to \emph{learn} the LCT parameters \((a,b,c)\) yields richer inductive biases and better compute/accuracy trade-offs.

\paragraph{Contributions.}  We:
\begin{enumerate}
  \item introduce the \textsc{LCT} layerâ€” a drop-in replacement for Fourier layers with analytical inverse;
  \item prove the discrete layer is unitary and reduces to several classical transforms as special cases;
  \item demonstrate throughput and perplexity gains on WikiText-103 and ImageNet-32 benchmarks.
\end{enumerate}