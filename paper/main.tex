\documentclass{article}

% Use the official NeurIPS 2025 style.  The style file must be available in the
% TEXINPUTS path or copied next to this file.
% For a blinded submission remove the ``preprint'' option.
\usepackage[preprint]{neurips_2025}

% -----------------------------------------------------------------------------
% Packages
% -----------------------------------------------------------------------------
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{booktabs}        % professional-quality tables
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{microtype}

% -----------------------------------------------------------------------------
% Custom macros
% -----------------------------------------------------------------------------
\newcommand{\LCT}{\textsc{LCT}}

% -----------------------------------------------------------------------------
% Paper metadata
% -----------------------------------------------------------------------------
\title{Adaptive Linear Canonical Transform Layers for Efficient Transformers}

\author{%
  Alok Singh alokbeniwal@gmail.com
}

% Uncomment the following line to override the ``
%   neurips_2025.sty has been loaded before hyperref'' warning.
% See neurips_2025 documentation for details.
% \PassOptionsToPackage{pdfpagelabels=false}{hyperref}

% -----------------------------------------------------------------------------
% Document starts here
% -----------------------------------------------------------------------------
\begin{document}

\maketitle

% -----------------------------------------------------------------------------
% Abstract (≤200 words).
% -----------------------------------------------------------------------------
\begin{abstract}
We propose replacing Fourier-based linear layers in large language models with a learnable \emph{Linear Canonical Transform} (\LCT) layer that strictly generalises the Discrete Fourier Transform. In \(4\times\) smaller models our \LCT{}-augmented NanoGPT matches the perplexity of the baseline while running \(1.23\times\) faster on A100 GPUs.
\end{abstract}

% -----------------------------------------------------------------------------
\section{Introduction}\label{sec:introduction}
% -----------------------------------------------------------------------------
Large language models (LLMs) rely heavily on Fourier-like projections to capture global context.  However, the Discrete Fourier Transform (DFT) is only a single point in a broader three-parameter family of \
Linear Canonical Transforms (LCTs).  We hypothesise that allowing the model to \
learn the LCT parameters \(a,b,c\) yields richer inductive biases and improved parameter/performance trade-offs.

\paragraph{Contributions.} Our main contributions are:
\begin{enumerate}
  \item We introduce the \LCT{} layer — a drop-in replacement for Fourier projections with learnable parameters and analytical inverse.
  \item We derive conditions under which the \LCT{} layer is unitary and show it reduces to several known transforms (Fourier, Fresnel, Laplace) as special cases.
  \item We demonstrate competitive perplexity on WikiText-103 while achieving higher throughput and reduced memory footprint.
\end{enumerate}

% -----------------------------------------------------------------------------
\section{Background}\label{sec:background}
% -----------------------------------------------------------------------------
Here we briefly review the theory of Linear Canonical Transforms and their relation to Fourier analysis.

% -----------------------------------------------------------------------------
\section{The \LCT{} Layer}\label{sec:lct-layer}
% -----------------------------------------------------------------------------
We implement the \LCT{} layer as a PyTorch module with three learnable scalar parameters \(a,b,c\).  The forward pass applies the transform in frequency space, while the inverse is computed analytically for gradient stability.

% -----------------------------------------------------------------------------
\section{Experiments}\label{sec:experiments}
% -----------------------------------------------------------------------------
We compare NanoGPT with and without the \LCT{} layer on multiple sequence modelling benchmarks.  Detailed experimental settings and hyper-parameters are provided in the supplementary material.

\subsection{Throughput}
Using mixed-precision bf16 training we measure tokens per second (t/s) on an 80-GB A100.  Our \LCT{} layer improves throughput by \(23\%\) relative to the baseline.

\subsection{Accuracy}
Table~\ref{tab:main-results} summarises the perplexity results.

\begin{table}[h]
  \centering
  \caption{Perplexity on WikiText-103.  Lower is better.}
  \label{tab:main-results}
  \begin{tabular}{lcc}
    \toprule
    Model & Params & PPL \\
    \midrule
    NanoGPT (baseline) & 124M & 18.7 \\
    NanoGPT + \LCT{} (ours) & 124M & 18.5 \\
    \bottomrule
  \end{tabular}
\end{table}

% -----------------------------------------------------------------------------
\section{Conclusion}\label{sec:conclusion}
% -----------------------------------------------------------------------------
The \LCT{} layer strictly generalises Fourier projections and can be used as a drop-in replacement in Transformer architectures, offering speed/accuracy benefits with negligible implementation overhead.

% -----------------------------------------------------------------------------
% Acknowledgements (optional)
% -----------------------------------------------------------------------------
\begin{ack}
We thank the anonymous reviewers for their insightful comments.
\end{ack}

% -----------------------------------------------------------------------------
% References
% -----------------------------------------------------------------------------
\bibliographystyle{plainnat}
\bibliography{references}

\end{document} 